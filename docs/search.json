[
  {
    "objectID": "esquisse_presentation.html#introduction-to-esquisse",
    "href": "esquisse_presentation.html#introduction-to-esquisse",
    "title": "Exploring the esquisse Package",
    "section": "Introduction to esquisse",
    "text": "Introduction to esquisse\n\nThe esquisse package helps in creating ggplot2 plots using a user-friendly interface.\nIt creates data visualizations that are quick and easy for anyone seeking a quick interactive analysis whether they are:\n\nnew to R or\nprefer a more visual approach to creating plots."
  },
  {
    "objectID": "esquisse_presentation.html#example-fruit-dataset",
    "href": "esquisse_presentation.html#example-fruit-dataset",
    "title": "Exploring the esquisse Package",
    "section": "Example: Fruit Dataset",
    "text": "Example: Fruit Dataset\n\n# Create a simple data frame for fruits\nfruit_data &lt;- data.frame(\n  fruit = c(\"apple\", \"banana\", \"strawberry\", \"mango\", \"avocado\"),\n  price = c(1.2, 0.5, 2.5, 3.0, 1.5),  # Price per pound in USD\n  quantity = c(50, 100, 40, 20, 30),   # Quantity available\n  calories = c(52, 89, 50, 277, 73),   # Calories per 100g\n  vitamin_c = c(4.6, 8.7, 7.0, 0.0, 36.0)    # Vitamin C content in mg per 100g\n)\n\nfruit_data\n\n       fruit price quantity calories vitamin_c\n1      apple   1.2       50       52       4.6\n2     banana   0.5      100       89       8.7\n3 strawberry   2.5       40       50       7.0\n4      mango   3.0       20      277       0.0\n5    avocado   1.5       30       73      36.0"
  },
  {
    "objectID": "esquisse_presentation.html#example-1-bar-chart-of-fruit-prices",
    "href": "esquisse_presentation.html#example-1-bar-chart-of-fruit-prices",
    "title": "Exploring the esquisse Package",
    "section": "Example 1: Bar Chart of Fruit Prices",
    "text": "Example 1: Bar Chart of Fruit Prices"
  },
  {
    "objectID": "esquisse_presentation.html#visualizing-the-fruit-data-set",
    "href": "esquisse_presentation.html#visualizing-the-fruit-data-set",
    "title": "Exploring the esquisse Package",
    "section": "Visualizing the Fruit Data set",
    "text": "Visualizing the Fruit Data set\nUsing esquisse, we can create a bar chart to compare the prices of different fruits.\n\nlibrary(esquisse)\nesquisser(data = fruit_data)"
  },
  {
    "objectID": "esquisse_presentation.html#code-given",
    "href": "esquisse_presentation.html#code-given",
    "title": "Exploring the esquisse Package",
    "section": "Code given:",
    "text": "Code given:\n\nlibrary(ggplot2)\n\nggplot(fruit_data) +\n aes(x = fruit, y = price) +\n geom_col(fill = \"#112446\") +\n theme_minimal()"
  },
  {
    "objectID": "esquisse_presentation.html#features-of-esquisse",
    "href": "esquisse_presentation.html#features-of-esquisse",
    "title": "Exploring the esquisse Package",
    "section": "Features of esquisse",
    "text": "Features of esquisse\n\nDrag and drop interface.\nCreate plots by dragging variables onto axes."
  },
  {
    "objectID": "esquisse_presentation.html#customization",
    "href": "esquisse_presentation.html#customization",
    "title": "Exploring the esquisse Package",
    "section": "Customization",
    "text": "Customization\n\nEasily customize colors, themes, and titles without writing a code.\nFilter data variables\nCreate different ggplot2 charts.\nExport graphs as PNG or PowerPoint file.\nExport the code to create the same graph"
  },
  {
    "objectID": "esquisse_presentation.html#we-can-play-with-the-colors-and-add-legend",
    "href": "esquisse_presentation.html#we-can-play-with-the-colors-and-add-legend",
    "title": "Exploring the esquisse Package",
    "section": "We can play with the colors and add legend:",
    "text": "We can play with the colors and add legend:"
  },
  {
    "objectID": "esquisse_presentation.html#code-given-1",
    "href": "esquisse_presentation.html#code-given-1",
    "title": "Exploring the esquisse Package",
    "section": "Code given:",
    "text": "Code given:\n\nggplot(fruit_data) +\n aes(x = fruit, y = price, fill = price) +\n geom_col() +\n scale_fill_gradient(low = \"#35011B\", \n high = \"#EF7373\") +\n labs(x = \"Fruit\", y = \"Price\", title = \"Fruit vs Price\", fill = \"Price\") +\n theme_linedraw()"
  },
  {
    "objectID": "esquisse_presentation.html#example-2-scatter-plot-of-price-vs.-calories",
    "href": "esquisse_presentation.html#example-2-scatter-plot-of-price-vs.-calories",
    "title": "Exploring the esquisse Package",
    "section": "Example 2: Scatter Plot of Price vs. Calories",
    "text": "Example 2: Scatter Plot of Price vs. Calories\nWe can create a scatter plot to examine the relationship between price and calories."
  },
  {
    "objectID": "esquisse_presentation.html#code-given-2",
    "href": "esquisse_presentation.html#code-given-2",
    "title": "Exploring the esquisse Package",
    "section": "Code given:",
    "text": "Code given:\n\nlibrary(ggplot2)\n\nggplot(fruit_data) +\n aes(x = price, y = calories, fill = fruit) +\n geom_point(size = 2.15, colour = \"#112446\") +\n scale_fill_hue(direction = 1) +\n labs(x = \"Price\", y = \"Calories\", title = \"Price vs Calories\", fill = \"Fruit\") +\n theme_gray() +\n theme(legend.text = element_text(size = 15L), legend.title = element_text(size = 15L))"
  },
  {
    "objectID": "esquisse_presentation.html#conclusion",
    "href": "esquisse_presentation.html#conclusion",
    "title": "Exploring the esquisse Package",
    "section": "Conclusion",
    "text": "Conclusion\n\nesquisse simplifies the process of creating complex visualizations.\nIdeal for users who may not be familiar with coding but want to leverage the power of ggplot2.\n\n\n\n\n\nhttps://quarto.org"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "mini_project2.html",
    "href": "mini_project2.html",
    "title": "Mini Project 2:Data Scraping",
    "section": "",
    "text": "# check that scraping is allowed (Step 0)\nrobotstxt::paths_allowed(\"https://fbref.com/en/comps/1/schedule/World-Cup-Scores-and-Fixtures\")\n\n\n fbref.com                      \n\n\n[1] TRUE\n\n# Step 1: read_html()\nscores &lt;- read_html(\"https://fbref.com/en/comps/1/schedule/World-Cup-Scores-and-Fixtures\")\n\n# step 2: html_nodes()\ntables &lt;- \n  html_nodes(scores, css = \"table\") \ntables\n\n{xml_nodeset (3)}\n[1] &lt;table class=\"stats_table sortable min_width\" id=\"sched_all\" data-cols-to ...\n[2] &lt;table class=\"stats_table sortable min_width\" id=\"sched_2022_1_1\" data-co ...\n[3] &lt;table class=\"stats_table sortable min_width\" id=\"sched_2022_1_2\" data-co ...\n\n# 3: html_table()\nhtml_table(tables, header = TRUE, fill = TRUE)\n\n[[1]]\n# A tibble: 67 × 15\n   Round     Wk Day   Date  Time  Home     xG Score    xG Away  Attendance Venue\n   &lt;chr&gt;  &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;\n 1 Group…     1 Sun   2022… 19:00 Qata…   0.3 0–2     1.2 ec E… 67,372     Al B…\n 2 Group…     1 Mon   2022… 16:00 Engl…   2.1 6–2     1.4 ir I… 45,334     Khal…\n 3 Group…     1 Mon   2022… 19:00 Sene…   0.9 0–2     0.7 nl N… 41,721     Al T…\n 4 Group…     1 Mon   2022… 22:00 Unit…   0.8 1–1     1.5 wls … 43,418     Ahme…\n 5 Group…     1 Tue   2022… 13:00 Arge…   2.2 1–2     0.1 sa S… 88,012     Lusa…\n 6 Group…     1 Tue   2022… 16:00 Denm…   1.4 0–0     0.9 tn T… 42,925     Educ…\n 7 Group…     1 Tue   2022… 19:00 Mexi…   0.7 0–0     0.9 pl P… 39,369     Stad…\n 8 Group…     1 Tue   2022… 22:00 Fran…   4   4–1     0.5 au A… 40,875     Al J…\n 9 Group…     1 Wed   2022… 13:00 Moro…   0.4 0–0     0.5 hr C… 59,407     Al B…\n10 Group…     1 Wed   2022… 16:00 Germ…   3.1 1–2     1.5 jp J… 42,608     Khal…\n# ℹ 57 more rows\n# ℹ 3 more variables: Referee &lt;chr&gt;, `Match Report` &lt;chr&gt;, Notes &lt;chr&gt;\n\n[[2]]\n# A tibble: 50 × 14\n      Wk Day   Date       Time  Home       xG Score    xG Away  Attendance Venue\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;\n 1     1 Sun   2022-11-20 19:00 Qatar …   0.3 0–2     1.2 ec E… 67,372     Al B…\n 2     1 Mon   2022-11-21 16:00 Englan…   2.1 6–2     1.4 ir I… 45,334     Khal…\n 3     1 Mon   2022-11-21 19:00 Senega…   0.9 0–2     0.7 nl N… 41,721     Al T…\n 4     1 Mon   2022-11-21 22:00 United…   0.8 1–1     1.5 wls … 43,418     Ahme…\n 5     1 Tue   2022-11-22 13:00 Argent…   2.2 1–2     0.1 sa S… 88,012     Lusa…\n 6     1 Tue   2022-11-22 16:00 Denmar…   1.4 0–0     0.9 tn T… 42,925     Educ…\n 7     1 Tue   2022-11-22 19:00 Mexico…   0.7 0–0     0.9 pl P… 39,369     Stad…\n 8     1 Tue   2022-11-22 22:00 France…   4   4–1     0.5 au A… 40,875     Al J…\n 9     1 Wed   2022-11-23 13:00 Morocc…   0.4 0–0     0.5 hr C… 59,407     Al B…\n10     1 Wed   2022-11-23 16:00 German…   3.1 1–2     1.5 jp J… 42,608     Khal…\n# ℹ 40 more rows\n# ℹ 3 more variables: Referee &lt;chr&gt;, `Match Report` &lt;chr&gt;, Notes &lt;lgl&gt;\n\n[[3]]\n# A tibble: 20 × 14\n   Round        Day   Date  Time  Home     xG Score    xG Away  Attendance Venue\n   &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;\n 1 \"Round of 1… \"Sat\" \"202… \"18:… \"Net…   1.7 \"3–1\"   1.5 \"us … \"44,846\"   \"Kha…\n 2 \"Round of 1… \"Sat\" \"202… \"22:… \"Arg…   1.6 \"2–1\"   0.6 \"au … \"45,032\"   \"Ahm…\n 3 \"Round of 1… \"Sun\" \"202… \"18:… \"Fra…   1.4 \"3–1\"   1.7 \"pl … \"40,989\"   \"Al …\n 4 \"Round of 1… \"Sun\" \"202… \"22:… \"Eng…   0.9 \"3–0\"   0.7 \"sn … \"65,985\"   \"Al …\n 5 \"Round of 1… \"Mon\" \"202… \"18:… \"Jap…   1.2 \"(1)…   1.4 \"hr … \"42,523\"   \"Al …\n 6 \"Round of 1… \"Mon\" \"202… \"22:… \"Bra…   3.6 \"4–1\"   0.5 \"kr … \"43,847\"   \"Sta…\n 7 \"Round of 1… \"Tue\" \"202… \"18:… \"Mor…   0.7 \"(3)…   1   \"es … \"44,667\"   \"Edu…\n 8 \"Round of 1… \"Tue\" \"202… \"22:… \"Por…   2.3 \"6–1\"   1.1 \"ch … \"83,720\"   \"Lus…\n 9 \"\"           \"\"    \"\"    \"\"    \"\"     NA   \"\"     NA   \"\"    \"\"         \"\"   \n10 \"Quarter-fi… \"Fri\" \"202… \"18:… \"Cro…   0.6 \"(4)…   2.5 \"br … \"43,893\"   \"Edu…\n11 \"Quarter-fi… \"Fri\" \"202… \"22:… \"Net…   0.6 \"(3)…   1.9 \"ar … \"88,235\"   \"Lus…\n12 \"Quarter-fi… \"Sat\" \"202… \"18:… \"Mor…   1.4 \"1–0\"   0.9 \"pt … \"44,198\"   \"Al …\n13 \"Quarter-fi… \"Sat\" \"202… \"22:… \"Eng…   2.4 \"1–2\"   0.9 \"fr … \"68,895\"   \"Al …\n14 \"\"           \"\"    \"\"    \"\"    \"\"     NA   \"\"     NA   \"\"    \"\"         \"\"   \n15 \"Semi-final… \"Tue\" \"202… \"22:… \"Arg…   2.3 \"3–0\"   0.5 \"hr … \"88,966\"   \"Lus…\n16 \"Semi-final… \"Wed\" \"202… \"22:… \"Fra…   2   \"2–0\"   0.9 \"ma … \"68,294\"   \"Al …\n17 \"\"           \"\"    \"\"    \"\"    \"\"     NA   \"\"     NA   \"\"    \"\"         \"\"   \n18 \"Third-plac… \"Sat\" \"202… \"18:… \"Cro…   0.7 \"2–1\"   1.2 \"ma … \"44,137\"   \"Kha…\n19 \"\"           \"\"    \"\"    \"\"    \"\"     NA   \"\"     NA   \"\"    \"\"         \"\"   \n20 \"Final\"      \"Sun\" \"202… \"18:… \"Arg…   3.2 \"(4)…   2.2 \"fr … \"88,966\"   \"Lus…\n# ℹ 3 more variables: Referee &lt;chr&gt;, `Match Report` &lt;chr&gt;, Notes &lt;chr&gt;\n\nscores_data1 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[1]]  \nscores_data1\n\n# A tibble: 67 × 15\n   Round     Wk Day   Date  Time  Home     xG Score    xG Away  Attendance Venue\n   &lt;chr&gt;  &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;\n 1 Group…     1 Sun   2022… 19:00 Qata…   0.3 0–2     1.2 ec E… 67,372     Al B…\n 2 Group…     1 Mon   2022… 16:00 Engl…   2.1 6–2     1.4 ir I… 45,334     Khal…\n 3 Group…     1 Mon   2022… 19:00 Sene…   0.9 0–2     0.7 nl N… 41,721     Al T…\n 4 Group…     1 Mon   2022… 22:00 Unit…   0.8 1–1     1.5 wls … 43,418     Ahme…\n 5 Group…     1 Tue   2022… 13:00 Arge…   2.2 1–2     0.1 sa S… 88,012     Lusa…\n 6 Group…     1 Tue   2022… 16:00 Denm…   1.4 0–0     0.9 tn T… 42,925     Educ…\n 7 Group…     1 Tue   2022… 19:00 Mexi…   0.7 0–0     0.9 pl P… 39,369     Stad…\n 8 Group…     1 Tue   2022… 22:00 Fran…   4   4–1     0.5 au A… 40,875     Al J…\n 9 Group…     1 Wed   2022… 13:00 Moro…   0.4 0–0     0.5 hr C… 59,407     Al B…\n10 Group…     1 Wed   2022… 16:00 Germ…   3.1 1–2     1.5 jp J… 42,608     Khal…\n# ℹ 57 more rows\n# ℹ 3 more variables: Referee &lt;chr&gt;, `Match Report` &lt;chr&gt;, Notes &lt;chr&gt;\n\n\n\n# Rename duplicate columns to make them unique\ncolnames &lt;- names(scores_data1) &lt;- make.unique(names(scores_data1))\n\n# Clean data\nscores_data1 &lt;- scores_data1[, 1:(ncol(scores_data1) - 2)] %&gt;% \n  select(-xG, -xG.1) %&gt;% \n  rename(week = Wk) %&gt;% \n  rename_with(~ str_to_lower(.), everything()) %&gt;% \n  slice(-c(17, 34, 51)) %&gt;% \n  mutate(away = str_remove(away, \"^[A-Za-z]+ \"),\n         home = str_remove(home, \" [A-Za-z]+$\"),\n         venue = str_remove(venue, \" \\\\(Neutral Site\\\\)\"))\n\nscores_data1\n\n# A tibble: 64 × 11\n   round       week day   date  time  home  score away  attendance venue referee\n   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  \n 1 Group sta…     1 Sun   2022… 19:00 Qatar 0–2   Ecua… 67,372     Al B… Daniel…\n 2 Group sta…     1 Mon   2022… 16:00 Engl… 6–2   IR I… 45,334     Khal… Raphae…\n 3 Group sta…     1 Mon   2022… 19:00 Sene… 0–2   Neth… 41,721     Al T… Wilton…\n 4 Group sta…     1 Mon   2022… 22:00 Unit… 1–1   Wales 43,418     Ahme… Abdulr…\n 5 Group sta…     1 Tue   2022… 13:00 Arge… 1–2   Saud… 88,012     Lusa… Slavko…\n 6 Group sta…     1 Tue   2022… 16:00 Denm… 0–0   Tuni… 42,925     Educ… César …\n 7 Group sta…     1 Tue   2022… 19:00 Mexi… 0–0   Pola… 39,369     Stad… Chris …\n 8 Group sta…     1 Tue   2022… 22:00 Fran… 4–1   Aust… 40,875     Al J… Victor…\n 9 Group sta…     1 Wed   2022… 13:00 Moro… 0–0   Croa… 59,407     Al B… Fernan…\n10 Group sta…     1 Wed   2022… 16:00 Germ… 1–2   Japan 42,608     Khal… Iván B…\n# ℹ 54 more rows"
  },
  {
    "objectID": "interactive_maps_github.html",
    "href": "interactive_maps_github.html",
    "title": "Mini Project 1B: Interactive Maps",
    "section": "",
    "text": "library(sf)\nstates &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")\nclass(states)\nstates\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\nSimple feature collection with 52 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -188.9049 ymin: 17.92956 xmax: -65.6268 ymax: 71.35163\nGeodetic CRS:  WGS 84\n# A tibble: 52 × 4\n   id    name                  density                                  geometry\n   &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;                        &lt;MULTIPOLYGON [°]&gt;\n 1 01    Alabama                 94.6  (((-87.3593 35.00118, -85.60667 34.98475…\n 2 02    Alaska                   1.26 (((-131.602 55.11798, -131.5692 55.28229…\n 3 04    Arizona                 57.0  (((-109.0425 37.00026, -109.048 31.33163…\n 4 05    Arkansas                56.4  (((-94.47384 36.50186, -90.15254 36.4963…\n 5 06    California             242.   (((-123.2333 42.00619, -122.3789 42.0116…\n 6 08    Colorado                49.3  (((-107.9197 41.00391, -105.729 40.99843…\n 7 09    Connecticut            739.   (((-73.05353 42.03905, -71.79931 42.0226…\n 8 10    Delaware               464.   (((-75.41409 39.80446, -75.5072 39.68396…\n 9 11    District of Columbia 10065    (((-77.03526 38.99387, -76.90929 38.8952…\n10 12    Florida                353.   (((-85.49714 30.99754, -85.00421 31.0030…\n# ℹ 42 more rows"
  },
  {
    "objectID": "interactive_maps_github.html#r-markdown",
    "href": "interactive_maps_github.html#r-markdown",
    "title": "Mini Project 1B: Interactive Maps",
    "section": "R Markdown",
    "text": "R Markdown\n\nAn Interactive Numeric Map: The Highest Violent Crime Rate Type in each State.\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(leaflet)\nlibrary(shiny)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncrime &lt;- read.csv(\"https://corgis-edu.github.io/corgis/datasets/csv/state_crime/state_crime.csv\")\n\nviolent_crimes &lt;- crime\n\nstates &lt;- states %&gt;%\n   left_join(violent_crimes, \n             by = c(\"name\" = \"State\"))\n\nviolent_crimes &lt;- violent_crimes %&gt;%\n  mutate(State = tolower(State))\n\ndata_mapped &lt;- violent_crimes %&gt;%\n   left_join(states, \n             by = c(\"State\" = \"name\"))\n\nstates_map &lt;- map_data(\"state\")\n\n\n# Summarize violent crimes by type\nviolent_crimes &lt;- crime %&gt;%\n  group_by(State) %&gt;% \n  summarize(Data.Totals.Violent.Assault = sum(Data.Totals.Violent.Assault),\n            Data.Totals.Violent.Rape = sum(Data.Totals.Violent.Rape),\n            Data.Totals.Violent.Robbery = sum(Data.Totals.Violent.Robbery),\n            Data.Totals.Violent.Murder = sum(Data.Totals.Violent.Murder)) %&gt;% \n  select(State, \n          Data.Totals.Violent.Assault, \n          Data.Totals.Violent.Rape, \n          Data.Totals.Violent.Robbery, \n          Data.Totals.Violent.Murder) %&gt;%\n   mutate(Highest_Crime = pmax(Data.Totals.Violent.Assault, \n                               Data.Totals.Violent.Rape, \n                               Data.Totals.Violent.Robbery,\n                               Data.Totals.Violent.Murder, na.rm = TRUE)) %&gt;%\n   mutate(Highest_Crime_Type = case_when(\n     Highest_Crime == Data.Totals.Violent.Assault ~ \"Assault\",\n     Highest_Crime == Data.Totals.Violent.Rape ~ \"Rape\",\n     Highest_Crime == Data.Totals.Violent.Robbery ~ \"Robbery\",\n     Highest_Crime == Data.Totals.Violent.Murder ~ \"Murder\",\n     TRUE ~ NA_character_\n   )) %&gt;%\n   select(State, \n          Highest_Crime_Type)\n\n\ndata_mapped &lt;- violent_crimes %&gt;%\n   left_join(states,\n             by = c(\"State\" = \"name\"))\n \nstates &lt;- states %&gt;%\n   left_join(violent_crimes, by = c(\"name\" = \"State\"))\n\n\n# Color mapping for different crime types\nstates &lt;- states %&gt;%\n   mutate(c = case_when(\n      Highest_Crime_Type == \"Assault\" ~ \"darkred\",\n      Highest_Crime_Type == \"Rape\" ~ \"white\",\n      Highest_Crime_Type == \"Robbery\" ~ \"lightyellow\",\n      Highest_Crime_Type == \"Murder\" ~ \"white\",\n      TRUE ~ \"grey\"\n))\n \n\nviolent_crimes &lt;- violent_crimes %&gt;%\n   mutate(State = tolower(State))\n\n\n# leaflet  \nleaflet(states) %&gt;%\n   setView(lng = -96, \n           lat = 37.8, \n           zoom = 4) %&gt;% \n   addTiles() %&gt;%\n   addPolygons(\n     fillColor = states$c,\n     weight = 1,\n     opacity = 1,\n     color = \"black\",\n     dashArray = \"3\",\n     fillOpacity = 0.7,\n     highlight = highlightOptions(\n       weight = 2,\n       color = \"white\",\n       dashArray = \"\",\n       fillOpacity = 0.7,\n       bringToFront = TRUE\n     ),\n     label = paste(states$name, \":\", states$Highest_Crime_Type),\n     labelOptions = labelOptions(\n       style = list(\"font-weight\" = \"normal\", \n                    padding = \"3px 8px\"),\n       textsize = \"15px\",\n       direction = \"auto\"))\n\nWarning in sf::st_is_longlat(x): bounding box has potentially an invalid value\nrange for longlat data\n\n\n\n\n\n\nDescription: An interactive numeric map of the United States (excluding Alaska and Hawaii) which visualizes the most common violent crime by state, with different colors representing types of crimes: red for assault, blue for rape, green for robbery, purple for murder, and grey where data is unavailable. The map labels each state with the crime type it experiences most frequently. The aim is to visualize the geographical distribution of violent crimes across the country, which helps in identify regional patterns that might influence decisions on crime prevention and safety precautions."
  },
  {
    "objectID": "mini_project1B.html",
    "href": "mini_project1B.html",
    "title": "Mini Project 1A: Static Maps",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mdsr)\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(dplyr)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n\n\nA Static Numeric Map: Total Violent Crime Rate by State(Excluding Alaska and Hawaii)\n\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(maps)\nlibrary(dplyr)\n\n\ncrime &lt;- read.csv(\"https://corgis-edu.github.io/corgis/datasets/csv/state_crime/state_crime.csv\")\n\n\ncrime_data &lt;- crime %&gt;%\n  select(State, Data.Rates.Violent.All)\n\n\ncrime_data &lt;- crime_data %&gt;%\n  filter(!State %in% c(\"Alaska\", \n                       \"Hawaii\"))\n\n\ncrime_data$State &lt;- tolower(crime_data$State)\n\n# US map data\nus_states &lt;- map_data(\"state\")\n\n# Merging data\nmap_data_merged &lt;- merge(us_states, \n                         crime_data, \n                         by.x = \"region\", \n                         by.y = \"State\", \n                         all.x = TRUE)\n\n# Creating the map\nggplot(map_data_merged, \n       aes(x = long, \n           y = lat, \n           group = group, \n           fill = Data.Rates.Violent.All)) +\n  geom_polygon(color = \"black\") + \n  scale_fill_continuous(name = \"Violent Crime Rate\", \n                        low = \"lightyellow\", \n                        high = \"darkred\", \n                        na.value = \"grey\") +\n  theme_minimal() +\n  labs(title = \"Total Violent Crime Rate by State (Excluding Alaska & Hawaii)\", \n       x = \"Longitude\", \n       y = \"Latitude\",\n       caption = \"Source: Corgis State Crime Dataset\") +\n  theme(plot.title = element_text(hjust = 0.5, \n                                  face = \"bold\", \n                                  size = 14),\n        plot.subtitle = element_text(hjust = 0.5, \n                                     size = 12))\n\n\n\n\n\n\n\n\nAlt-text: This is a static map that visualizes the total violent crime rate by state in the United States, excluding Alaska and Hawaii. Each state is colored according to its violent crime rate per 100,000 people, with a gradient scale transitioning from light yellow (lower crime rates) to dark red (higher crime rates). The plot is titled “Total Violent Crime Rate by State (Excluding Alaska and Hawaii),” and there is a legend on the side indicating the color scale for crime rates. An annotation at the bottom notes the data source as the CORGIS State Crime Dataset.\nThis map makes it easy to identify areas with high and low crime rates and the purpose of it is to see the regional differences in violent crime rates which can be used in discussions of public safety and law policies. States colored in darker shades of red indicate a higher rate of violent crime, leading to potentially looking into the underlying causes. Additionally, the exclusion of Alaska and Hawaii allows for a focused comparison among the contiguous states. We can see that all states are of varied shades of light yellow, which is not that helpful when it comes to figuring out the state with the highest and lowest. This could be improved by looking at violent crime rates in a specific time span.\n\nA Static Categorical Map: Most Common Violent Crime by State (Excluding Alaska & Hawaii ˆ\n\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(maps)\nlibrary(dplyr)\n\n# Load the crime dataset\ncrime &lt;- read.csv(\"https://corgis-edu.github.io/corgis/datasets/csv/state_crime/state_crime.csv\")\n\n# Exclude Alaska and Hawaii and select relevant columns\nviolent_crimes &lt;- crime %&gt;%\n  filter(!State %in% c(\"Alaska\", \"Hawaii\")) %&gt;%\n  select(State, Data.Totals.Violent.Assault, Data.Totals.Violent.Rape, \n         Data.Totals.Violent.Robbery, Data.Totals.Violent.Murder)\n\n\n# Convert state names to lowercase for joining with map data\nviolent_crimes &lt;- violent_crimes %&gt;%\n  mutate(State = tolower(State))\n\n# Find the most common violent crime type for each state\nviolent_crimes &lt;- violent_crimes %&gt;%\n  mutate(Highest_Crime_Type = case_when(\n    Data.Totals.Violent.Assault &gt;= Data.Totals.Violent.Rape & \n    Data.Totals.Violent.Assault &gt;= Data.Totals.Violent.Robbery & \n    Data.Totals.Violent.Assault &gt;= Data.Totals.Violent.Murder ~ \"Assault\",\n    \n    Data.Totals.Violent.Rape &gt;= Data.Totals.Violent.Assault & \n    Data.Totals.Violent.Rape &gt;= Data.Totals.Violent.Robbery & \n    Data.Totals.Violent.Rape &gt;= Data.Totals.Violent.Murder ~ \"Rape\",\n    \n    Data.Totals.Violent.Robbery &gt;= Data.Totals.Violent.Assault & \n    Data.Totals.Violent.Robbery &gt;= Data.Totals.Violent.Rape & \n    Data.Totals.Violent.Robbery &gt;= Data.Totals.Violent.Murder ~ \"Robbery\",\n    \n    Data.Totals.Violent.Murder &gt;= Data.Totals.Violent.Assault & \n    Data.Totals.Violent.Murder &gt;= Data.Totals.Violent.Rape & \n    Data.Totals.Violent.Murder &gt;= Data.Totals.Violent.Robbery ~ \"Murder\",\n    \n    TRUE ~ NA_character_\n  ))\n\n# Load US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with crime data\nmap_data_merged &lt;- merge(us_states, violent_crimes, by.x = \"region\", by.y = \"State\", all.x = TRUE)\n\n# Create a categorical static map\nggplot(map_data_merged, aes(x = long, y = lat, group = group, fill = Highest_Crime_Type)) +\n  geom_polygon(color = \"black\") + \n  scale_fill_manual(values = c(\"Assault\" = \"lightblue\", \"Rape\" = \"antiquewhite2\", \"Robbery\" = \"cornflowerblue\", \"Murder\" = \"burlywood3\"), \n                    na.value = \"grey\", name = \"Most Common Crime Type\") +\n  theme_minimal() +\n  labs(title = \"Most Common Violent Crime by State (Excluding Alaska & Hawaii)\",\n       subtitle = \"Categorical Map of the U.S. Showing the Most Prevalent Type of Violent Crime\",\n       x = \"Longitude\", \n       y = \"Latitude\",\n       caption = \"Source: Corgis State Crime Dataset\") +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n        plot.subtitle = element_text(hjust = 0.5, size = 12),\n        legend.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\nDescription: For this specific map, I intended it to be categorical but not interactive. This is an easy map to show the most common violent crime act in every state where red is assault, blue is rape and green is robbery. We can see that assault dominates most of the united states while robbery(green) dominates 7 states only. There is zero domination of rape crimes. I like this map since it’s very straight forward, I would want to add abbreviations of each state to make it easier for the viewers to detect where every crime is dominant."
  },
  {
    "objectID": "mini_project1B.html#r-markdown",
    "href": "mini_project1B.html#r-markdown",
    "title": "Mini Project 1A: Static Maps",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mdsr)\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(dplyr)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n\n\nA Static Numeric Map: Total Violent Crime Rate by State(Excluding Alaska and Hawaii)\n\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(maps)\nlibrary(dplyr)\n\n\ncrime &lt;- read.csv(\"https://corgis-edu.github.io/corgis/datasets/csv/state_crime/state_crime.csv\")\n\n\ncrime_data &lt;- crime %&gt;%\n  select(State, Data.Rates.Violent.All)\n\n\ncrime_data &lt;- crime_data %&gt;%\n  filter(!State %in% c(\"Alaska\", \n                       \"Hawaii\"))\n\n\ncrime_data$State &lt;- tolower(crime_data$State)\n\n# US map data\nus_states &lt;- map_data(\"state\")\n\n# Merging data\nmap_data_merged &lt;- merge(us_states, \n                         crime_data, \n                         by.x = \"region\", \n                         by.y = \"State\", \n                         all.x = TRUE)\n\n# Creating the map\nggplot(map_data_merged, \n       aes(x = long, \n           y = lat, \n           group = group, \n           fill = Data.Rates.Violent.All)) +\n  geom_polygon(color = \"black\") + \n  scale_fill_continuous(name = \"Violent Crime Rate\", \n                        low = \"lightyellow\", \n                        high = \"darkred\", \n                        na.value = \"grey\") +\n  theme_minimal() +\n  labs(title = \"Total Violent Crime Rate by State (Excluding Alaska & Hawaii)\", \n       x = \"Longitude\", \n       y = \"Latitude\",\n       caption = \"Source: Corgis State Crime Dataset\") +\n  theme(plot.title = element_text(hjust = 0.5, \n                                  face = \"bold\", \n                                  size = 14),\n        plot.subtitle = element_text(hjust = 0.5, \n                                     size = 12))\n\n\n\n\n\n\n\n\nAlt-text: This is a static map that visualizes the total violent crime rate by state in the United States, excluding Alaska and Hawaii. Each state is colored according to its violent crime rate per 100,000 people, with a gradient scale transitioning from light yellow (lower crime rates) to dark red (higher crime rates). The plot is titled “Total Violent Crime Rate by State (Excluding Alaska and Hawaii),” and there is a legend on the side indicating the color scale for crime rates. An annotation at the bottom notes the data source as the CORGIS State Crime Dataset.\nThis map makes it easy to identify areas with high and low crime rates and the purpose of it is to see the regional differences in violent crime rates which can be used in discussions of public safety and law policies. States colored in darker shades of red indicate a higher rate of violent crime, leading to potentially looking into the underlying causes. Additionally, the exclusion of Alaska and Hawaii allows for a focused comparison among the contiguous states. We can see that all states are of varied shades of light yellow, which is not that helpful when it comes to figuring out the state with the highest and lowest. This could be improved by looking at violent crime rates in a specific time span.\n\nA Static Categorical Map: Most Common Violent Crime by State (Excluding Alaska & Hawaii ˆ\n\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(maps)\nlibrary(dplyr)\n\n# Load the crime dataset\ncrime &lt;- read.csv(\"https://corgis-edu.github.io/corgis/datasets/csv/state_crime/state_crime.csv\")\n\n# Exclude Alaska and Hawaii and select relevant columns\nviolent_crimes &lt;- crime %&gt;%\n  filter(!State %in% c(\"Alaska\", \"Hawaii\")) %&gt;%\n  select(State, Data.Totals.Violent.Assault, Data.Totals.Violent.Rape, \n         Data.Totals.Violent.Robbery, Data.Totals.Violent.Murder)\n\n\n# Convert state names to lowercase for joining with map data\nviolent_crimes &lt;- violent_crimes %&gt;%\n  mutate(State = tolower(State))\n\n# Find the most common violent crime type for each state\nviolent_crimes &lt;- violent_crimes %&gt;%\n  mutate(Highest_Crime_Type = case_when(\n    Data.Totals.Violent.Assault &gt;= Data.Totals.Violent.Rape & \n    Data.Totals.Violent.Assault &gt;= Data.Totals.Violent.Robbery & \n    Data.Totals.Violent.Assault &gt;= Data.Totals.Violent.Murder ~ \"Assault\",\n    \n    Data.Totals.Violent.Rape &gt;= Data.Totals.Violent.Assault & \n    Data.Totals.Violent.Rape &gt;= Data.Totals.Violent.Robbery & \n    Data.Totals.Violent.Rape &gt;= Data.Totals.Violent.Murder ~ \"Rape\",\n    \n    Data.Totals.Violent.Robbery &gt;= Data.Totals.Violent.Assault & \n    Data.Totals.Violent.Robbery &gt;= Data.Totals.Violent.Rape & \n    Data.Totals.Violent.Robbery &gt;= Data.Totals.Violent.Murder ~ \"Robbery\",\n    \n    Data.Totals.Violent.Murder &gt;= Data.Totals.Violent.Assault & \n    Data.Totals.Violent.Murder &gt;= Data.Totals.Violent.Rape & \n    Data.Totals.Violent.Murder &gt;= Data.Totals.Violent.Robbery ~ \"Murder\",\n    \n    TRUE ~ NA_character_\n  ))\n\n# Load US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with crime data\nmap_data_merged &lt;- merge(us_states, violent_crimes, by.x = \"region\", by.y = \"State\", all.x = TRUE)\n\n# Create a categorical static map\nggplot(map_data_merged, aes(x = long, y = lat, group = group, fill = Highest_Crime_Type)) +\n  geom_polygon(color = \"black\") + \n  scale_fill_manual(values = c(\"Assault\" = \"lightblue\", \"Rape\" = \"antiquewhite2\", \"Robbery\" = \"cornflowerblue\", \"Murder\" = \"burlywood3\"), \n                    na.value = \"grey\", name = \"Most Common Crime Type\") +\n  theme_minimal() +\n  labs(title = \"Most Common Violent Crime by State (Excluding Alaska & Hawaii)\",\n       subtitle = \"Categorical Map of the U.S. Showing the Most Prevalent Type of Violent Crime\",\n       x = \"Longitude\", \n       y = \"Latitude\",\n       caption = \"Source: Corgis State Crime Dataset\") +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n        plot.subtitle = element_text(hjust = 0.5, size = 12),\n        legend.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\nDescription: For this specific map, I intended it to be categorical but not interactive. This is an easy map to show the most common violent crime act in every state where red is assault, blue is rape and green is robbery. We can see that assault dominates most of the united states while robbery(green) dominates 7 states only. There is zero domination of rape crimes. I like this map since it’s very straight forward, I would want to add abbreviations of each state to make it easier for the viewers to detect where every crime is dominant."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rania Abdul Hafiz",
    "section": "",
    "text": "Hi there! My name is Rania Abdul Hafiz. I am a student at St. Olaf College majoring in Kinesiology and Statistics and Data Science concentration. I work as a Sports Medicine Assistant and I like to use my data science skills to explore various trends in the health field and specifically sports injuries. Poke around my website to learn more!"
  },
  {
    "objectID": "mini_project4/mini_project4.html",
    "href": "mini_project4/mini_project4.html",
    "title": "Mini Project 4: Text Analysis",
    "section": "",
    "text": "This mini project focuses on exploring the lyrics of Kendrick Lamar’s songs through various text analysis methods, including sentiment analysis, term frequency-inverse document frequency (TF-IDF), and word clouds."
  },
  {
    "objectID": "mini_project4/mini_project4.html#overview",
    "href": "mini_project4/mini_project4.html#overview",
    "title": "Mini Project 4: Text Analysis",
    "section": "",
    "text": "This mini project focuses on exploring the lyrics of Kendrick Lamar’s songs through various text analysis methods, including sentiment analysis, term frequency-inverse document frequency (TF-IDF), and word clouds."
  },
  {
    "objectID": "mini_project4/mini_project4.html#loading-all-packages-needed",
    "href": "mini_project4/mini_project4.html#loading-all-packages-needed",
    "title": "Mini Project 4: Text Analysis",
    "section": "Loading all packages needed",
    "text": "Loading all packages needed\n\nlibrary(billboard)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(wordcloud)\nlibrary(wordcloud2)\nlibrary(knitr)\nlibrary(sentimentr)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(RColorBrewer)"
  },
  {
    "objectID": "mini_project4/mini_project4.html#loading-data",
    "href": "mini_project4/mini_project4.html#loading-data",
    "title": "Mini Project 4: Text Analysis",
    "section": "Loading data",
    "text": "Loading data\n\nkendrick_data &lt;- read_csv(\"~/raniaabdulhafiz.github.io/mini_project4/kendrick_data.csv\")\ncurse_words &lt;- read_csv(\"~/raniaabdulhafiz.github.io/mini_project4/curse_words.csv\")"
  },
  {
    "objectID": "mini_project4/mini_project4.html#using-str_functions-and-regular-expressions-tidy-the-data",
    "href": "mini_project4/mini_project4.html#using-str_functions-and-regular-expressions-tidy-the-data",
    "title": "Mini Project 4: Text Analysis",
    "section": "Using str_functions and regular expressions, tidy the data",
    "text": "Using str_functions and regular expressions, tidy the data\n\ntidy_lyrics &lt;- kendrick_data %&gt;%\n  unnest_tokens(word, lyrics) %&gt;% #separate lyrics by word\n  anti_join(curse_words) %&gt;% \n  filter(!word %in% stop_words$word) %&gt;% #filter out stop words and create n \n  group_by(word) %&gt;% \n  mutate(n = n()) %&gt;% \n  distinct()\n\ntidy_lyrics\n\n# A tibble: 15,805 × 10\n# Groups:   word [6,268]\n   track_name album release_date duration_ms popularity speechiness danceability\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;             &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Growing A… Over… 2010-09-14        220995         52       0.366        0.586\n 2 Growing A… Over… 2010-09-14        220995         52       0.366        0.586\n 3 Growing A… Over… 2010-09-14        220995         52       0.366        0.586\n 4 Growing A… Over… 2010-09-14        220995         52       0.366        0.586\n 5 Growing A… Over… 2010-09-14        220995         52       0.366        0.586\n 6 Growing A… Over… 2010-09-14        220995         52       0.366        0.586\n 7 Growing A… Over… 2010-09-14        220995         52       0.366        0.586\n 8 Growing A… Over… 2010-09-14        220995         52       0.366        0.586\n 9 Growing A… Over… 2010-09-14        220995         52       0.366        0.586\n10 Growing A… Over… 2010-09-14        220995         52       0.366        0.586\n# ℹ 15,795 more rows\n# ℹ 3 more variables: tempo &lt;dbl&gt;, word &lt;chr&gt;, n &lt;int&gt;\n\n\n\ntidy_lyrics &lt;- tidy_lyrics %&gt;% \n  mutate(across(everything(), ~ str_to_lower(.))) %&gt;% #change all to lower case\n  filter(!str_detect(word, \"\\\\d\"), #remove all words that contain numbers\n         !str_detect(word, \"[^a-zA-Z0-9 ]\"), #remove all words that contain anything not a letter or number\n         str_length(word) &gt; 3) #keep words longer than 3 letters \n  \ntidy_lyrics\n\n# A tibble: 13,887 × 10\n# Groups:   word [5,649]\n   track_name album release_date duration_ms popularity speechiness danceability\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;       \n 1 growing a… over… 2010-09-14   220995      52         0.366       0.586       \n 2 growing a… over… 2010-09-14   220995      52         0.366       0.586       \n 3 growing a… over… 2010-09-14   220995      52         0.366       0.586       \n 4 growing a… over… 2010-09-14   220995      52         0.366       0.586       \n 5 growing a… over… 2010-09-14   220995      52         0.366       0.586       \n 6 growing a… over… 2010-09-14   220995      52         0.366       0.586       \n 7 growing a… over… 2010-09-14   220995      52         0.366       0.586       \n 8 growing a… over… 2010-09-14   220995      52         0.366       0.586       \n 9 growing a… over… 2010-09-14   220995      52         0.366       0.586       \n10 growing a… over… 2010-09-14   220995      52         0.366       0.586       \n# ℹ 13,877 more rows\n# ℹ 3 more variables: tempo &lt;chr&gt;, word &lt;chr&gt;, n &lt;chr&gt;"
  },
  {
    "objectID": "mini_project4/mini_project4.html#sentiment-analysis---bing-sentiment",
    "href": "mini_project4/mini_project4.html#sentiment-analysis---bing-sentiment",
    "title": "Mini Project 4: Text Analysis",
    "section": "Sentiment analysis - bing sentiment",
    "text": "Sentiment analysis - bing sentiment\nBy performing sentiment analysis using the bing lexicon, the bar plot showing the net sentiment of each album gives a glimpse of the emotional tone of Kendrick’s music. I measured the positive and negative sentiments, and calculated the net sentiment by subtracting the number of negative words from the positive ones. We can see a pattern in the emotional tone of Kendrick’s albums over time, which indicating shifts in mood, themes, or messages from one album to the next. Rappers tend to release something new each time that reflects their personal experiences, but we tend to see a domination of positive tone and use of words in most of his albums.\n\nbing_sentiment &lt;- tidy_lyrics %&gt;%\n  inner_join(get_sentiments(\"bing\"), by = \"word\") %&gt;%\n  count(album, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(net_sentiment = positive - negative)\n\n\nggplot(bing_sentiment, aes(x = album, \n                           y = net_sentiment, \n                           fill = album)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Net Sentiment of Kendrick Lamar Albums\", \n       x = \"Album\", \n       y = \"Net Sentiment\")"
  },
  {
    "objectID": "mini_project4/mini_project4.html#tf_idf",
    "href": "mini_project4/mini_project4.html#tf_idf",
    "title": "Mini Project 4: Text Analysis",
    "section": "tf_idf",
    "text": "tf_idf\nThe TF-IDF plot identifies the most important and distinctive words in each album. Words with high TF-IDF values are unique to specific albums, revealing central concepts to each album’s lyrics. We can see that Kendrick’s focus shifts on particular subjects, whether political, social, or personal, across different stages of his career. This gives a deeper understanding of his artistic evolution and shifting in priorities. For instance, on his Diss Tracks, we can see that the word “audience” was used a lot, while in his album Untitled Unmastered, there was a mix of emotions “crying”, “mortal”, “hooray”. There is a shift in emotions based on the type of the album and the targeted audience.\n\ntf_idf &lt;- tidy_lyrics %&gt;%\n  count(album, word, sort = TRUE) %&gt;%\n  bind_tf_idf(word, album, n) %&gt;%\n  arrange(desc(tf_idf))\n\n\ntf_idf %&gt;%\n  group_by(album) %&gt;%\n  slice_max(tf_idf, n = 10) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = reorder_within(word, tf_idf, album), \n             y = tf_idf, fill = album)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~album, scales = \"free\") +\n  coord_flip() +\n  scale_x_reordered() +\n  theme_minimal() +\n  labs(title = \"Top Words by TF-IDF in Kendrick Lamar Albums\", \n       y = \"TF-IDF\", \n       x = \"Word\")"
  },
  {
    "objectID": "mini_project4/mini_project4.html#wordcloud",
    "href": "mini_project4/mini_project4.html#wordcloud",
    "title": "Mini Project 4: Text Analysis",
    "section": "Wordcloud",
    "text": "Wordcloud\nThe word cloud shows the most frequent words in Kendrick Lamar’s lyrics. Larger words appear more frequently, while smaller words are less common, the number of time a word is repeated is shown as well. This visual makes it easy to identify dominant themes in Kendrick’s lyrics. Words like “life,” “feel,” “free,” and “world” could appear large, indicating their frequent use in his lyrics, which could point to overarching themes in his music. The color and shape of the word cloud are added as a way of visually engaging the viewer.\n\ntidy_lyrics %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  wordcloud2(size = 1.5, color = \"random-light\", backgroundColor = \"black\")"
  }
]